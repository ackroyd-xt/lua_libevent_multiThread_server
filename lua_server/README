

架构： 多线程线程池 + libevent + lua
应用场景：
	1. 客户端和服务端的交互有多个步骤（lua）
	2. 可能在服务端由于非网络问题陷入阻塞(例如查数据库)（多线程）
	3. 活跃连接数远小于总连接数（libevent + couroutine）

优势：
	1. 业务逻辑与server代码分离
	2. 服务端可以阻塞（典型单线程libevent的服务端不可以阻塞，否则整个server会停下来）
	3. 支持超大连接数（libevent + coroutine, 具体数值需要测试）
	4. 不低于libevent的性能
	5. sever逻辑可编程
	6. ?
缺陷：
	1. 需要较大的 C API 库,且写server逻辑代码时往往要加厚此胶水层
	2. ?

todo:
	1. 更丰富的 C API 提供给lua使用(readn, writen, 非阻塞write， etc;同时使用lua进行包装）; API与C代码分离？
	2. 任务队列？
	3. 客户端的编程和API （目前只有服务端可以编程)	(高性能connector,用于测试server性能）
	4. 引入pipe，各个线程使用pipe通信，把pipe加入epoll/poll?用于多步骤的任务?
	5. 数据库处理的异步化



2. 怎么取消lua胶水层
			具体应用哪种看场景，若Ｃ的处理相当复杂且难以用编写就直接使用CPS； 若处理较简单、但逻辑较复杂就用胶水层。
	-----写个自动包装C函数的类？

	------resume和callback的部分已经完成。。也就是说目前其实已经完成了类似libevent的callback机制（使用resume完成），现在的问题是yield。考虑yield的位置：只有在需要等待的时候才会yield。也就是说调用yield的时候一定是在等待客户端的答复或者等待某个定时器。这样就可以考虑包装一个专门用于等待的函数？例如
		int cyield() {
			state = lua_getctx(Ctx);
			if(state == LUA_OK) {//第一次调用
				return lua_yieldk(L, 1, 1, cyield);//ctx+1, 指定resume后仍调用此函数
			}
			return; //第二次调用，也就是第一次resume后进入此函数返回
		}
	
		int process() {
			...;//业务处理
			send1();
			cyield();
			send2();
			if(msg==illegal) {
				sendErr();
				cyield();//等待重发正确报文
			}else
				cyield();
			insert_database();
			return;
		}
	
		------不行，第二次返回时直接执行cyield然后就会退出(因为丢掉了调用cyield的函数的函数帧)
			

xthread的改造： 
	3. 线程可以根据拿到的context切换为不同类型的线程；或者由主线程将不同类型的context分配给不同的线程


x_lua的任务：
1. 目前的threadpool，单个thread的逻辑较简单，更改为可以支持coroutine（一个线程对应多个coroutine，例如对单个连接的数据处理需要花较长时间时，可以处理其他连接的任务？(例如数据库处理？
-----1. 网络io时间较长  2. 报文处理时间较长    
	如何将多线程与select融合？
	应用多线程的场景：monp_coll，需要向预处理层发起请求,等待答复然后进行处理。其实也就是异步io的另一种实现,只不过避免了对每个应答查找其对应请求；同时增加了线程等待回复时的等待时间。
	----socket场景：
	1. 报文转发(空充，全品牌划扣，oamAgent_alm	----------多线程或异步io均可（多线程本身也相当于异步）
	2. snmp请求处理(agentMain		----------------------查询可能比较花时间，直接单进程单线程处理即可（CPU密集型）
	3. 发送webservice请求（monp_coll	------------------多线程或异步IO （避免一个请求等待时间影响另一个请求）
	4. memcache/redis/???			----------------------单进程单线程即可，多线程是牺牲性能换取吞吐量
	5. 清账单中心(提供网络服务）	----------------------单进程单线程

	每个连接需要复杂的交互的场景。
	socket处理丢给专门负责socket的线程。
	业务处理由线程池完成。

